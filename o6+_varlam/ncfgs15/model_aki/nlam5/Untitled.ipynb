{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class sde_Brownian in module GPy.kern.src.sde_brownian:\n",
      "\n",
      "class sde_Brownian(GPy.kern.src.brownian.Brownian)\n",
      " |  sde_Brownian(*args, **kw)\n",
      " |  \n",
      " |  Class provide extra functionality to transfer this covariance function into\n",
      " |  SDE form.\n",
      " |  \n",
      " |  Linear kernel:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     k(x,y) = \\sigma^2 min(x,y)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      sde_Brownian\n",
      " |      GPy.kern.src.brownian.Brownian\n",
      " |      GPy.kern.src.kern.Kern\n",
      " |      GPy.core.parameterization.parameterized.Parameterized\n",
      " |      paramz.parameterized.Parameterized\n",
      " |      GPy.core.parameterization.priorizable.Priorizable\n",
      " |      paramz.core.parameter_core.Parameterizable\n",
      " |      paramz.core.parameter_core.OptimizationHandlable\n",
      " |      paramz.core.constrainable.Constrainable\n",
      " |      paramz.core.indexable.Indexable\n",
      " |      paramz.core.nameable.Nameable\n",
      " |      paramz.core.gradcheckable.Gradcheckable\n",
      " |      paramz.core.pickleable.Pickleable\n",
      " |      paramz.core.parentable.Parentable\n",
      " |      paramz.core.updateable.Updateable\n",
      " |      paramz.core.observable.Observable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  sde(self)\n",
      " |      Return the state space representation of the covariance.\n",
      " |  \n",
      " |  sde_update_gradient_full(self, gradients)\n",
      " |      Update gradient in the order in which parameters are represented in the\n",
      " |      kernel\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GPy.kern.src.brownian.Brownian:\n",
      " |  \n",
      " |  K(self, X, X2=None)\n",
      " |      Compute the kernel function.\n",
      " |      \n",
      " |      .. math::\n",
      " |          K_{ij} = k(X_i, X_j)\n",
      " |      \n",
      " |      :param X: the first set of inputs to the kernel\n",
      " |      :param X2: (optional) the second set of arguments to the kernel. If X2\n",
      " |                 is None, this is passed throgh to the 'part' object, which\n",
      " |                 handLes this as X2 == X.\n",
      " |  \n",
      " |  Kdiag(self, X)\n",
      " |      The diagonal of the kernel matrix K\n",
      " |      \n",
      " |      .. math::\n",
      " |          Kdiag_{i} = k(X_i, X_i)\n",
      " |  \n",
      " |  __init__(self, input_dim=1, variance=1.0, active_dims=None, name='Brownian')\n",
      " |      The base class for a kernel: a positive definite function\n",
      " |      which forms of a covariance function (kernel).\n",
      " |      \n",
      " |      input_dim:\n",
      " |      \n",
      " |          is the number of dimensions to work on. Make sure to give the\n",
      " |          tight dimensionality of inputs.\n",
      " |          You most likely want this to be the integer telling the number of\n",
      " |          input dimensions of the kernel.\n",
      " |      \n",
      " |      active_dims:\n",
      " |      \n",
      " |          is the active_dimensions of inputs X we will work on.\n",
      " |          All kernels will get sliced Xes as inputs, if _all_dims_active is not None\n",
      " |          Only positive integers are allowed in active_dims!\n",
      " |          if active_dims is None, slicing is switched off and all X will be passed through as given.\n",
      " |      \n",
      " |      :param int input_dim: the number of input dimensions to the function\n",
      " |      :param array-like|None active_dims: list of indices on which dimensions this kernel works on, or none if no slicing\n",
      " |      \n",
      " |      Do not instantiate.\n",
      " |  \n",
      " |  update_gradients_full(self, dL_dK, X, X2=None)\n",
      " |      Set the gradients of all parameters when doing full (N) inference.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GPy.kern.src.kern.Kern:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Overloading of the '+' operator. for more control, see self.add\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |      Here we overload the '*' operator. See self.prod for more information\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |      Here we overload the '*' operator. See self.prod for more information\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |      Shortcut for tensor `prod`.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add(self, other, name='sum')\n",
      " |      Add another kernel to this one.\n",
      " |      \n",
      " |      :param other: the other kernel to be added\n",
      " |      :type other: GPy.kern\n",
      " |  \n",
      " |  get_most_significant_input_dimensions(self, which_indices=None)\n",
      " |      Determine which dimensions should be plotted\n",
      " |      \n",
      " |      Returns the top three most signification input dimensions\n",
      " |      \n",
      " |      if less then three dimensions, the non existing dimensions are\n",
      " |      labeled as None, so for a 1 dimensional input this returns\n",
      " |      (0, None, None).\n",
      " |      \n",
      " |      :param which_indices: force the indices to be the given indices.\n",
      " |      :type which_indices: int or tuple(int,int) or tuple(int,int,int)\n",
      " |  \n",
      " |  gradients_X(self, dL_dK, X, X2)\n",
      " |      .. math::\n",
      " |      \n",
      " |          \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial K}\\frac{\\partial K}{\\partial X}\n",
      " |  \n",
      " |  gradients_XX(self, dL_dK, X, X2, cov=True)\n",
      " |      .. math::\n",
      " |      \n",
      " |          \\frac{\\partial^2 L}{\\partial X\\partial X_2} = \\frac{\\partial L}{\\partial K}\\frac{\\partial^2 K}{\\partial X\\partial X_2}\n",
      " |  \n",
      " |  gradients_XX_diag(self, dL_dKdiag, X, cov=True)\n",
      " |      The diagonal of the second derivative w.r.t. X and X2\n",
      " |  \n",
      " |  gradients_X_X2(self, dL_dK, X, X2)\n",
      " |  \n",
      " |  gradients_X_diag(self, dL_dKdiag, X)\n",
      " |      The diagonal of the derivative w.r.t. X\n",
      " |  \n",
      " |  gradients_Z_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior, psi0=None, psi1=None, psi2=None)\n",
      " |      Returns the derivative of the objective wrt Z, using the chain rule\n",
      " |      through the expectation variables.\n",
      " |  \n",
      " |  gradients_qX_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior)\n",
      " |      Compute the gradients wrt the parameters of the variational\n",
      " |      distruibution q(X), chain-ruling via the expectations of the kernel\n",
      " |  \n",
      " |  input_sensitivity(self, summarize=True)\n",
      " |      Returns the sensitivity for each dimension of this kernel.\n",
      " |      \n",
      " |      This is an arbitrary measurement based on the parameters\n",
      " |      of the kernel per dimension and scaling in general.\n",
      " |      \n",
      " |      Use this as relative measurement, not for absolute comparison between\n",
      " |      kernels.\n",
      " |  \n",
      " |  plot = deprecate_plot(self, *args, **kwargs)\n",
      " |  \n",
      " |  plot_ARD(kernel, filtering=None, legend=False, canvas=None, **kwargs)\n",
      " |      If an ARD kernel is present, plot a bar representation using matplotlib\n",
      " |      \n",
      " |      :param fignum: figure number of the plot\n",
      " |      :param filtering: list of names, which to use for plotting ARD parameters.\n",
      " |                        Only kernels which match names in the list of names in filtering\n",
      " |                        will be used for plotting.\n",
      " |      :type filtering: list of names to use for ARD plot\n",
      " |  \n",
      " |  plot_covariance(kernel, x=None, label=None, plot_limits=None, visible_dims=None, resolution=None, projection='2d', levels=20, **kwargs)\n",
      " |      Plot a kernel covariance w.r.t. another x.\n",
      " |      \n",
      " |      :param array-like x: the value to use for the other kernel argument (kernels are a function of two variables!)\n",
      " |      :param plot_limits: the range over which to plot the kernel\n",
      " |      :type plot_limits: Either (xmin, xmax) for 1D or (xmin, xmax, ymin, ymax) / ((xmin, xmax), (ymin, ymax)) for 2D\n",
      " |      :param array-like visible_dims: input dimensions (!) to use for x. Make sure to select 2 or less dimensions to plot.\n",
      " |      :resolution: the resolution of the lines used in plotting. for 2D this defines the grid for kernel evaluation.\n",
      " |      :param {2d|3d} projection: What projection shall we use to plot the kernel?\n",
      " |      :param int levels: for 2D projection, how many levels for the contour plot to use?\n",
      " |      :param kwargs:  valid kwargs for your specific plotting library\n",
      " |  \n",
      " |  prod(self, other, name='mul')\n",
      " |      Multiply two kernels (either on the same space, or on the tensor\n",
      " |      product of the input space).\n",
      " |      \n",
      " |      :param other: the other kernel to be added\n",
      " |      :type other: GPy.kern\n",
      " |  \n",
      " |  psi0(self, Z, variational_posterior)\n",
      " |      .. math::\n",
      " |          \\psi_0 = \\sum_{i=0}^{n}E_{q(X)}[k(X_i, X_i)]\n",
      " |  \n",
      " |  psi1(self, Z, variational_posterior)\n",
      " |      .. math::\n",
      " |          \\psi_1^{n,m} = E_{q(X)}[k(X_n, Z_m)]\n",
      " |  \n",
      " |  psi2(self, Z, variational_posterior)\n",
      " |      .. math::\n",
      " |          \\psi_2^{m,m'} = \\sum_{i=0}^{n}E_{q(X)}[ k(Z_m, X_i) k(X_i, Z_{m'})]\n",
      " |  \n",
      " |  psi2n(self, Z, variational_posterior)\n",
      " |      .. math::\n",
      " |          \\psi_2^{n,m,m'} = E_{q(X)}[ k(Z_m, X_n) k(X_n, Z_{m'})]\n",
      " |      \n",
      " |      Thus, we do not sum out n, compared to psi2\n",
      " |  \n",
      " |  reset_gradients(self)\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |  \n",
      " |  update_gradients_diag(self, dL_dKdiag, X)\n",
      " |      update the gradients of all parameters when using only the diagonal elements of the covariance matrix\n",
      " |  \n",
      " |  update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior)\n",
      " |      Set the gradients of all parameters when doing inference with\n",
      " |      uncertain inputs, using expectations of the kernel.\n",
      " |      \n",
      " |      The essential maths is\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          \\frac{\\partial L}{\\partial \\theta_i} & = \\frac{\\partial L}{\\partial \\psi_0}\\frac{\\partial \\psi_0}{\\partial \\theta_i}\\\n",
      " |              & \\quad + \\frac{\\partial L}{\\partial \\psi_1}\\frac{\\partial \\psi_1}{\\partial \\theta_i}\\\n",
      " |              & \\quad + \\frac{\\partial L}{\\partial \\psi_2}\\frac{\\partial \\psi_2}{\\partial \\theta_i}\n",
      " |      \n",
      " |      Thus, we push the different derivatives through the gradients of the psi\n",
      " |      statistics. Be sure to set the gradients for all kernel\n",
      " |      parameters here.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from GPy.kern.src.kern.Kern:\n",
      " |  \n",
      " |  from_dict(input_dict)\n",
      " |      Instantiate an object of a derived class using the information\n",
      " |      in input_dict (built by the to_dict method of the derived class).\n",
      " |      More specifically, after reading the derived class from input_dict,\n",
      " |      it calls the method _build_from_input_dict of the derived class.\n",
      " |      Note: This method should not be overrided in the derived class. In case\n",
      " |      it is needed, please override _build_from_input_dict instate.\n",
      " |      \n",
      " |      :param dict input_dict: Dictionary with all the information needed to\n",
      " |         instantiate the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GPy.core.parameterization.parameterized.Parameterized:\n",
      " |  \n",
      " |  randomize(self, rand_gen=None, *args, **kwargs)\n",
      " |      Randomize the model.\n",
      " |      Make this draw from the prior if one exists, else draw from given random generator\n",
      " |      \n",
      " |      :param rand_gen: np random number generator which takes args and kwargs\n",
      " |      :param flaot loc: loc parameter for random number generator\n",
      " |      :param float scale: scale parameter for random number generator\n",
      " |      :param args, kwargs: will be passed through to random number generator\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.parameterized.Parameterized:\n",
      " |  \n",
      " |  __getitem__(self, name, paramlist=None)\n",
      " |  \n",
      " |  __setattr__(self, name, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setitem__(self, name, value, paramlist=None)\n",
      " |  \n",
      " |  __str__(self, header=True, VT100=True)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  build_pydot(self, G=None)\n",
      " |      Build a pydot representation of this model. This needs pydot installed.\n",
      " |      \n",
      " |      Example Usage::\n",
      " |      \n",
      " |          np.random.seed(1000)\n",
      " |          X = np.random.normal(0,1,(20,2))\n",
      " |          beta = np.random.uniform(0,1,(2,1))\n",
      " |          Y = X.dot(beta)\n",
      " |          m = RidgeRegression(X, Y)\n",
      " |          G = m.build_pydot()\n",
      " |          G.write_png('example_hierarchy_layout.png')\n",
      " |      \n",
      " |      The output looks like:\n",
      " |      \n",
      " |      .. image:: ./example_hierarchy_layout.png\n",
      " |      \n",
      " |      Rectangles are parameterized objects (nodes or leafs of hierarchy).\n",
      " |      \n",
      " |      Trapezoids are param objects, which represent the arrays for parameters.\n",
      " |      \n",
      " |      Black arrows show parameter hierarchical dependence. The arrow points\n",
      " |      from parents towards children.\n",
      " |      \n",
      " |      Orange arrows show the observer pattern. Self references (here) are\n",
      " |      the references to the call to parameters changed and references upwards\n",
      " |      are the references to tell the parents they need to update.\n",
      " |  \n",
      " |  copy(self, memo=None)\n",
      " |      Returns a (deep) copy of the current parameter handle.\n",
      " |      \n",
      " |      All connections to parents of the copy will be cut.\n",
      " |      \n",
      " |      :param dict memo: memo for deepcopy\n",
      " |      :param Parameterized which: parameterized object which started the copy process [default: self]\n",
      " |  \n",
      " |  get_property_string(self, propname)\n",
      " |  \n",
      " |  grep_param_names(self, regexp)\n",
      " |      create a list of parameters, matching regular expression regexp\n",
      " |  \n",
      " |  link_parameter(self, param, index=None)\n",
      " |      :param parameters:  the parameters to add\n",
      " |      :type parameters:   list of or one :py:class:`paramz.param.Param`\n",
      " |      :param [index]:     index of where to put parameters\n",
      " |      \n",
      " |      Add all parameters to this param class, you can insert parameters\n",
      " |      at any given index using the :func:`list.insert` syntax\n",
      " |  \n",
      " |  link_parameters(self, *parameters)\n",
      " |      convenience method for adding several\n",
      " |      parameters without gradient specification\n",
      " |  \n",
      " |  unlink_parameter(self, param)\n",
      " |      :param param: param object to remove from being a parameter of this parameterized object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.parameterized.Parameterized:\n",
      " |  \n",
      " |  flattened_parameters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GPy.core.parameterization.priorizable.Priorizable:\n",
      " |  \n",
      " |  log_prior(self)\n",
      " |      evaluate the prior\n",
      " |  \n",
      " |  set_prior(self, prior, warning=True)\n",
      " |      Set the prior for this object to prior.\n",
      " |      :param :class:`~GPy.priors.Prior` prior: a prior to set for this parameter\n",
      " |      :param bool warning: whether to warn if another prior was set for this parameter\n",
      " |  \n",
      " |  unset_priors(self, *priors)\n",
      " |      Un-set all priors given (in *priors) from this parameter handle.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.parameter_core.Parameterizable:\n",
      " |  \n",
      " |  disable_caching(self)\n",
      " |  \n",
      " |  enable_caching(self)\n",
      " |  \n",
      " |  initialize_parameter(self)\n",
      " |      Call this function to initialize the model, if you built it without initialization.\n",
      " |      \n",
      " |      This HAS to be called manually before optmizing or it will be causing\n",
      " |      unexpected behaviour, if not errors!\n",
      " |  \n",
      " |  parameters_changed(self)\n",
      " |      This method gets called when parameters have changed.\n",
      " |      Another way of listening to param changes is to\n",
      " |      add self as a listener to the param, such that\n",
      " |      updates get passed through. See :py:function:``paramz.param.Observable.add_observer``\n",
      " |  \n",
      " |  save(self, filename, ftype='HDF5')\n",
      " |      Save all the model parameters into a file (HDF5 by default).\n",
      " |      \n",
      " |      This is not supported yet. We are working on having a consistent,\n",
      " |      human readable way of saving and loading GPy models. This only\n",
      " |      saves the parameter array to a hdf5 file. In order\n",
      " |      to load the model again, use the same script for building the model\n",
      " |      you used to build this model. Then load the param array from this hdf5\n",
      " |      file and set the parameters of the created model:\n",
      " |      \n",
      " |          >>> m[:] = h5_file['param_array']\n",
      " |      \n",
      " |      This is less then optimal, we are working on a better solution to that.\n",
      " |  \n",
      " |  traverse(self, visit, *args, **kwargs)\n",
      " |      Traverse the hierarchy performing `visit(self, *args, **kwargs)`\n",
      " |      at every node passed by downwards. This function includes self!\n",
      " |      \n",
      " |      See *visitor pattern* in literature. This is implemented in pre-order fashion.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          #Collect all children:\n",
      " |      \n",
      " |          children = []\n",
      " |          self.traverse(children.append)\n",
      " |          print children\n",
      " |  \n",
      " |  traverse_parents(self, visit, *args, **kwargs)\n",
      " |      Traverse the hierarchy upwards, visiting all parents and their children except self.\n",
      " |      See \"visitor pattern\" in literature. This is implemented in pre-order fashion.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      parents = []\n",
      " |      self.traverse_parents(parents.append)\n",
      " |      print parents\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.core.parameter_core.Parameterizable:\n",
      " |  \n",
      " |  gradient\n",
      " |  \n",
      " |  num_params\n",
      " |      Return the number of parameters of this parameter_handle.\n",
      " |      Param objects will always return 0.\n",
      " |  \n",
      " |  param_array\n",
      " |      Array representing the parameters of this class.\n",
      " |      There is only one copy of all parameters in memory, two during optimization.\n",
      " |      \n",
      " |      !WARNING!: setting the parameter array MUST always be done in memory:\n",
      " |      m.param_array[:] = m_copy.param_array\n",
      " |  \n",
      " |  unfixed_param_array\n",
      " |      Array representing the parameters of this class.\n",
      " |      There is only one copy of all parameters in memory, two during optimization.\n",
      " |      \n",
      " |      !WARNING!: setting the parameter array MUST always be done in memory:\n",
      " |      m.param_array[:] = m_copy.param_array\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.parameter_core.OptimizationHandlable:\n",
      " |  \n",
      " |  parameter_names(self, add_self=False, adjust_for_printing=False, recursive=True, intermediate=False)\n",
      " |      Get the names of all parameters of this model or parameter. It starts\n",
      " |      from the parameterized object you are calling this method on.\n",
      " |      \n",
      " |      Note: This does not unravel multidimensional parameters,\n",
      " |            use parameter_names_flat to unravel parameters!\n",
      " |      \n",
      " |      :param bool add_self: whether to add the own name in front of names\n",
      " |      :param bool adjust_for_printing: whether to call `adjust_name_for_printing` on names\n",
      " |      :param bool recursive: whether to traverse through hierarchy and append leaf node names\n",
      " |      :param bool intermediate: whether to add intermediate names, that is parameterized objects\n",
      " |  \n",
      " |  parameter_names_flat(self, include_fixed=False)\n",
      " |      Return the flattened parameter names for all subsequent parameters\n",
      " |      of this parameter. We do not include the name for self here!\n",
      " |      \n",
      " |      If you want the names for fixed parameters as well in this list,\n",
      " |      set include_fixed to True.\n",
      " |          if not hasattr(obj, 'cache'):\n",
      " |              obj.cache = FunctionCacher()\n",
      " |      :param bool include_fixed: whether to include fixed names here.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.core.parameter_core.OptimizationHandlable:\n",
      " |  \n",
      " |  gradient_full\n",
      " |      Note to users:\n",
      " |      This does not return the gradient in the right shape! Use self.gradient\n",
      " |      for the right gradient array.\n",
      " |      \n",
      " |      To work on the gradient array, use this as the gradient handle.\n",
      " |      This method exists for in memory use of parameters.\n",
      " |      When trying to access the true gradient array, use this.\n",
      " |  \n",
      " |  optimizer_array\n",
      " |      Array for the optimizer to work on.\n",
      " |      This array always lives in the space for the optimizer.\n",
      " |      Thus, it is untransformed, going from Transformations.\n",
      " |      \n",
      " |      Setting this array, will make sure the transformed parameters for this model\n",
      " |      will be set accordingly. It has to be set with an array, retrieved from\n",
      " |      this method, as e.g. fixing will resize the array.\n",
      " |      \n",
      " |      The optimizer should only interfere with this array, such that transformations\n",
      " |      are secured.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.constrainable.Constrainable:\n",
      " |  \n",
      " |  constrain(self, transform, warning=True, trigger_parent=True)\n",
      " |      :param transform: the :py:class:`paramz.transformations.Transformation`\n",
      " |                        to constrain the this parameter to.\n",
      " |      :param warning: print a warning if re-constraining parameters.\n",
      " |      \n",
      " |      Constrain the parameter to the given\n",
      " |      :py:class:`paramz.transformations.Transformation`.\n",
      " |  \n",
      " |  constrain_bounded(self, lower, upper, warning=True, trigger_parent=True)\n",
      " |      :param lower, upper: the limits to bound this parameter to\n",
      " |      :param warning: print a warning if re-constraining parameters.\n",
      " |      \n",
      " |      Constrain this parameter to lie within the given range.\n",
      " |  \n",
      " |  constrain_fixed(self, value=None, warning=True, trigger_parent=True)\n",
      " |      Constrain this parameter to be fixed to the current value it carries.\n",
      " |      \n",
      " |      This does not override the previous constraints, so unfixing will\n",
      " |      restore the constraint set before fixing.\n",
      " |      \n",
      " |      :param warning: print a warning for overwriting constraints.\n",
      " |  \n",
      " |  constrain_negative(self, warning=True, trigger_parent=True)\n",
      " |      :param warning: print a warning if re-constraining parameters.\n",
      " |      \n",
      " |      Constrain this parameter to the default negative constraint.\n",
      " |  \n",
      " |  constrain_positive(self, warning=True, trigger_parent=True)\n",
      " |      :param warning: print a warning if re-constraining parameters.\n",
      " |      \n",
      " |      Constrain this parameter to the default positive constraint.\n",
      " |  \n",
      " |  fix = constrain_fixed(self, value=None, warning=True, trigger_parent=True)\n",
      " |  \n",
      " |  unconstrain(self, *transforms)\n",
      " |      :param transforms: The transformations to unconstrain from.\n",
      " |      \n",
      " |      remove all :py:class:`paramz.transformations.Transformation`\n",
      " |      transformats of this parameter object.\n",
      " |  \n",
      " |  unconstrain_bounded(self, lower, upper)\n",
      " |      :param lower, upper: the limits to unbound this parameter from\n",
      " |      \n",
      " |      Remove (lower, upper) bounded constrain from this parameter/\n",
      " |  \n",
      " |  unconstrain_fixed(self)\n",
      " |      This parameter will no longer be fixed.\n",
      " |      \n",
      " |      If there was a constraint on this parameter when fixing it,\n",
      " |      it will be constraint with that previous constraint.\n",
      " |  \n",
      " |  unconstrain_negative(self)\n",
      " |      Remove negative constraint of this parameter.\n",
      " |  \n",
      " |  unconstrain_positive(self)\n",
      " |      Remove positive constraint of this parameter.\n",
      " |  \n",
      " |  unfix = unconstrain_fixed(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.core.constrainable.Constrainable:\n",
      " |  \n",
      " |  is_fixed\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.indexable.Indexable:\n",
      " |  \n",
      " |  add_index_operation(self, name, operations)\n",
      " |      Add index operation with name to the operations given.\n",
      " |      \n",
      " |      raises: attribute error if operations exist.\n",
      " |  \n",
      " |  remove_index_operation(self, name)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.nameable.Nameable:\n",
      " |  \n",
      " |  hierarchy_name(self, adjust_for_printing=True)\n",
      " |      return the name for this object with the parents names attached by dots.\n",
      " |      \n",
      " |      :param bool adjust_for_printing: whether to call :func:`~adjust_for_printing()`\n",
      " |                                       on the names, recursively\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.core.nameable.Nameable:\n",
      " |  \n",
      " |  name\n",
      " |      The name of this object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.gradcheckable.Gradcheckable:\n",
      " |  \n",
      " |  checkgrad(self, verbose=0, step=1e-06, tolerance=0.001, df_tolerance=1e-12)\n",
      " |      Check the gradient of this parameter with respect to the highest parent's\n",
      " |      objective function.\n",
      " |      This is a three point estimate of the gradient, wiggling at the parameters\n",
      " |      with a stepsize step.\n",
      " |      The check passes if either the ratio or the difference between numerical and\n",
      " |      analytical gradient is smaller then tolerance.\n",
      " |      \n",
      " |      :param bool verbose: whether each parameter shall be checked individually.\n",
      " |      :param float step: the stepsize for the numerical three point gradient estimate.\n",
      " |      :param float tolerance: the tolerance for the gradient ratio or difference.\n",
      " |      :param float df_tolerance: the tolerance for df_tolerance\n",
      " |      \n",
      " |      .. note::\n",
      " |         The *dF_ratio* indicates the limit of accuracy of numerical gradients.\n",
      " |         If it is too small, e.g., smaller than 1e-12, the numerical gradients\n",
      " |         are usually not accurate enough for the tests (shown with blue).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.pickleable.Pickleable:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  pickle(self, f, protocol=-1)\n",
      " |      :param f: either filename or open file object to write to.\n",
      " |                if it is an open buffer, you have to make sure to close\n",
      " |                it properly.\n",
      " |      :param protocol: pickling protocol to use, python-pickle for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paramz.core.pickleable.Pickleable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.parentable.Parentable:\n",
      " |  \n",
      " |  has_parent(self)\n",
      " |      Return whether this parentable object currently has a parent.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.updateable.Updateable:\n",
      " |  \n",
      " |  toggle_update(self)\n",
      " |  \n",
      " |  trigger_update(self, trigger_parent=True)\n",
      " |      Update the model from the current state.\n",
      " |      Make sure that updates are on, otherwise this\n",
      " |      method will do nothing\n",
      " |      \n",
      " |      :param bool trigger_parent: Whether to trigger the parent, after self has updated\n",
      " |  \n",
      " |  update_model(self, updates=None)\n",
      " |      Get or set, whether automatic updates are performed. When updates are\n",
      " |      off, the model might be in a non-working state. To make the model work\n",
      " |      turn updates on again.\n",
      " |      \n",
      " |      :param bool|None updates:\n",
      " |      \n",
      " |          bool: whether to do updates\n",
      " |          None: get the current update state\n",
      " |  \n",
      " |  update_toggle(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paramz.core.observable.Observable:\n",
      " |  \n",
      " |  add_observer(self, observer, callble, priority=0)\n",
      " |      Add an observer `observer` with the callback `callble`\n",
      " |      and priority `priority` to this observers list.\n",
      " |  \n",
      " |  change_priority(self, observer, callble, priority)\n",
      " |  \n",
      " |  notify_observers(self, which=None, min_priority=None)\n",
      " |      Notifies all observers. Which is the element, which kicked off this\n",
      " |      notification loop. The first argument will be self, the second `which`.\n",
      " |      \n",
      " |      .. note::\n",
      " |         \n",
      " |         notifies only observers with priority p > min_priority!\n",
      " |         \n",
      " |      :param min_priority: only notify observers with priority > min_priority\n",
      " |                           if min_priority is None, notify all observers in order\n",
      " |  \n",
      " |  remove_observer(self, observer, callble=None)\n",
      " |      Either (if callble is None) remove all callables,\n",
      " |      which were added alongside observer,\n",
      " |      or remove callable `callble` which was added alongside\n",
      " |      the observer `observer`.\n",
      " |  \n",
      " |  set_updates(self, on=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GPy.kern.sde_Brownian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
